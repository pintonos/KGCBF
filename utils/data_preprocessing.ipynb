{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal, BNode\n",
    "from rdflib.namespace import FOAF, RDF, SDO, RDFS, OWL, DC\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_graph = Graph()\n",
    "# https://data.mendeley.com/datasets/zp23s23xpb/1\n",
    "input_folder = \"../dump/soler/\"\n",
    "for filename in tqdm(os.listdir(input_folder)[:1000]):\n",
    "    f = os.path.join(input_folder, filename)\n",
    "    try:\n",
    "        g1 = Graph().parse(f, format='xml')\n",
    "\n",
    "        #check for literals\n",
    "        for s, p, o in g1:\n",
    "            if not rdflib.term._is_valid_uri(o):\n",
    "                g1.set((s, p, Literal(o)))\n",
    "             \n",
    "        global_graph = global_graph + g1\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "global_graph.serialize(destination=\"../datasets/soler.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://ebiquity.umbc.edu/resource/html/id/82/foafPub-dataset\n",
    "g = Graph()\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"dc\", DC)\n",
    "\n",
    "with open(\"dump/foaf_pub/triple_person.sql\") as file:\n",
    "    for i, line in enumerate(tqdm(file)):\n",
    "        try:\n",
    "            parsed = sqlparse.parse(line)[0]\n",
    "            values = str(parsed.tokens[-3])[9:-1].split(',')\n",
    "\n",
    "            subject = values[5][1:-1]\n",
    "            predicate = values[4][1:-1]\n",
    "            object = values[3][1:-1]\n",
    "\n",
    "            #check all uris\n",
    "            if not (rdflib.term._is_valid_uri(subject) and rdflib.term._is_valid_uri(predicate)):\n",
    "                continue\n",
    "\n",
    "            subject = URIRef(subject)\n",
    "            predicate = URIRef(predicate)\n",
    "\n",
    "            # check if literal\n",
    "            if not rdflib.term._is_valid_uri(object):\n",
    "                object = Literal(object)\n",
    "            else:\n",
    "                object = URIRef(object)\n",
    "            \n",
    "            g.add((subject, predicate, object))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #if i > 9000:\n",
    "        #    break\n",
    "\n",
    "g.serialize(destination=\"../datasets/foaf_pub.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FactBench\n",
    "# https://github.com/DeFacto/FactBench\n",
    "global_graph = Graph()\n",
    "input_folder = \"dump/FactBench/\"\n",
    "for folder in tqdm(os.listdir(input_folder)):\n",
    "    folder = os.path.join(input_folder, folder)\n",
    "    for filename in os.listdir(folder):\n",
    "        f = os.path.join(folder, filename)\n",
    "        try:\n",
    "            g1 = Graph().parse(f, format='ttl')\n",
    "                \n",
    "            global_graph = global_graph + g1\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "global_graph.serialize(destination=\"../datasets/factbench.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, RDFXML\n",
    "from rdflib import Graph\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\") # use this endpoint for higher timeout\n",
    "\n",
    "# https://gist.github.com/tomsaleeba/ff8e145b3efd1127e48baa6512df24e2\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX dbpedia: <http://dbpedia.org/resource/>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "    CONSTRUCT {\n",
    "        ?actor ?p1 ?o1 .\n",
    "        ?s2 ?p2 ?actor .\n",
    "        ?movie ?p3 ?o2 .\n",
    "        ?s3 ?p4 ?movie .\n",
    "    }\n",
    "    WHERE {\n",
    "        ?actor ?p1 ?o1 {\n",
    "            SELECT ?actor, ?movie\n",
    "            WHERE {\n",
    "                ?movie dbpedia-owl:starring ?actor .\n",
    "            } \n",
    "            order by asc(UCASE(str(?actor)))\n",
    "            LIMIT 10 # set graph size with this, to high results in partial result only, because of timeout\n",
    "        } .\n",
    "        ?s2 ?p2 ?actor FILTER (?p2 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects)) . \n",
    "\n",
    "        ?movie ?p3 ?o2 FILTER (?p3 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects)) . \n",
    "        ?s3 ?p4 ?movie FILTER (?p4 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects)) . \n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "sparql.setReturnFormat(RDFXML)\n",
    "results = sparql.query().convert()\n",
    "results.serialize(destination=\"../datasets/dbpedia.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/tomsaleeba/ff8e145b3efd1127e48baa6512df24e2\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, RDFXML, JSON, TURTLE, XML\n",
    "from rdflib import Graph\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\") # use this endpoint for higher timeout\n",
    "\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX dbpedia: <http://dbpedia.org/resource/>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "    SELECT ?actor, ?movie\n",
    "    WHERE {\n",
    "        ?movie dbpedia-owl:starring ?actor .\n",
    "    }\n",
    "    #order by asc(UCASE(str(?actor)))\n",
    "    LIMIT 100 # set graph size with this\n",
    "\"\"\")\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "actors_movies = sparql.queryAndConvert()\n",
    "actors_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "for a_m in tqdm(actors_movies[\"results\"][\"bindings\"]):\n",
    "    actor = a_m['actor']['value'].rsplit('/', 1)[1]\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX dbpedia: <http://dbpedia.org/resource/>\n",
    "        PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "        CONSTRUCT {\n",
    "            dbpedia:\"\"\" + actor + \"\"\" ?p1 ?o1 .\n",
    "            ?s2 ?p2 dbpedia:\"\"\" + actor + \"\"\" .\n",
    "        }\n",
    "        WHERE {\n",
    "            dbpedia:\"\"\" + actor + \"\"\" ?p1 ?o1 .\n",
    "            ?s2 ?p2 dbpedia:\"\"\" + actor + \"\"\" FILTER (?p2 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects, foaf:primaryTopic)) .\n",
    "        }\n",
    "    \"\"\")\n",
    "    sparql.setReturnFormat(TURTLE)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        g = g + Graph().parse(data=results, format=\"turtle\")\n",
    "        g.add((URIRef(\"http://dbpedia.org/resource/\" + actor), RDF.type, URIRef(\"http://dbpedia.org/ontology/Actor\")))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "unique_movies = set([m['movie']['value'].rsplit('/', 1)[1].replace(\"(\", \"\\(\").replace(\")\", \"\\)\") for m in actors_movies[\"results\"][\"bindings\"]])\n",
    "for movie in tqdm(unique_movies):\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX dbpedia: <http://dbpedia.org/resource/>\n",
    "        PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "        CONSTRUCT {\n",
    "            dbpedia:\"\"\" + movie + \"\"\" ?p3 ?o2 .\n",
    "            #?s3 ?p4  dbpedia:\"\"\" + movie + \"\"\" .\n",
    "        }\n",
    "        WHERE {\n",
    "            dbpedia:\"\"\" + movie + \"\"\" ?p3 ?o2 FILTER (?p3 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects)) .\n",
    "            #?s3 ?p4  dbpedia:\"\"\" + movie + \"\"\" FILTER (?p4 NOT IN (dbo:wikiPageWikiLink, dbo:wikiPageRedirects)) .\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    sparql.setReturnFormat(TURTLE)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        g = g + Graph().parse(data=results, format=\"turtle\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.add((URIRef(\"http://dbpedia.org/ontology/starring\"), RDFS.domain, URIRef(\"http://dbpedia.org/ontology/work\")))\n",
    "g.add((URIRef(\"http://dbpedia.org/ontology/starring\"), RDFS.range, URIRef(\"http://dbpedia.org/ontology/Actor\")))\n",
    "g.serialize(destination=\"../datasets/dbpedia.ttl\", format=\"turtle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf2a14cc801ef9a174741e417f11c1b53a953174dbf66788c085a30508026a90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
