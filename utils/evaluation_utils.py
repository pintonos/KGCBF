import re

import yaml
from rdflib import Graph

from framework.logger import Logger


def matching_regex_count(content: str, query: str) -> int:
    """
    Returns the number of matching regex expression in the content.
    @param content: The content to check for regex expressions.
    @param query: The regex query.
    @return: The number of matching expressions.
    """
    return len(re.findall(query, content))


def matching_graph_count(graph: Graph, query: str) -> int:
    """
    Returns the number of matching SPARQL expressions in the knowledge graph.
    @param graph: The knowledge graph (rdflib graph).
    @param query: The SPARQL query.
    @return: The number of matching expressions.
    """
    return len(graph.query(query))


def fill_approach_dictionary_placeholders(query: str, error: dict, regex_escape: bool) -> str:
    """
    Replaces the placeholders in an approach dictionary query according to the given error.
    @param query: The query (containing placeholders) which is used to detect a given error.
    @param error: An error as logged in the error_log, containing the original and corrupted triple.
    @param regex_escape: Whether to escape the string for regex.
    @return: A query without placeholders which can be used to test if an approach has detected an error.
    """
    query = query.replace("$original.subject$", encode_uri(error['original']['s'], regex_escape))
    query = query.replace("$original.predicate$", encode_uri(error['original']['p'], regex_escape))
    query = query.replace("$original.object$", encode_uri(error['original']['o'], regex_escape))
    query = query.replace("$corrupt.subject$", encode_uri(error['corrupted']['s'], regex_escape))
    query = query.replace("$corrupt.predicate$", encode_uri(error['corrupted']['p'], regex_escape))
    query = query.replace("$corrupt.object$", encode_uri(error['corrupted']['o'], regex_escape))
    return query


def encode_uri(uri: str, regex_escape: bool) -> str:
    """
    Encodes a unique resource identifier string.
    @param uri: The name of a unique resource identifier.
    @param regex_escape: Whether to escape the string for regex.
    @return: The encoded string.
    """
    return f"<{uri}>" if not regex_escape else re.escape(f"<{uri}>")


def generate_approach_report(validator_name: str, error_log: Logger, validation_dict: dict,
                             approach_report_location: str, kgcbf_report_location="data/report.yaml") -> None:
    """
    This method generates the KGCBF report according to the defined parameters.
    The KGCBF report is stored in data/report.yaml by default.
    @param validator_name: The name of the validation approach.
    @param error_log: The error log generated by the corruptor module.
    @param validation_dict: The validation approach dictionary specifying how to detect specific errors.
    @param approach_report_location: The location of the report generated by the validation approach.
    @param kgcbf_report_location: (Optional) The location where the KGCBF report is stored.
    """
    # extract error dictionary from error log
    logged_errors = error_log.log_dict

    # set up basic report structure
    report = {
        'errors_detected': [],
        'errors_not_detected': [],
        'categories': {}
    }

    # get validation report source depending on type
    if "type" not in validation_dict:
        print(f"The provided validation dictionary does not contain a \"type\" property. "
              f"Please provide one. (Valid: graph,regex)")
        return
    if validation_dict["type"] == "graph":
        # if the output type is a graph, we parse the approach report to a rdflib graph.
        data_source = Graph()
        data_source = data_source.parse(approach_report_location)
        # To find errors, we use "matching_graph_count" (SPARQL queries)
        query_function = matching_graph_count
    elif validation_dict["type"] == "regex":
        # if the output type is regex, we read the approach report as a file.
        with open(approach_report_location, 'r') as f:
            data_source = f.read()
        # To find errors, we use "matching_regex_count" (regex queries)
        query_function = matching_regex_count
    else:
        # If the output type is not matching, we do not generate a KGCBF report.
        print(f"Skipping KGCBF report generation due to "
              f"unknown parse type \"{validation_dict['type']}\" (Valid: graph,regex)")
        return

    # get total number of detected errors, and total number of introduced errors
    # For this, we use a predefined pattern from the dictionary which appears once for every error.
    detected = query_function(data_source, validation_dict["total_errors"]["pattern"])
    introduced = len([err for cat in logged_errors for err in logged_errors[cat]])

    # Next, we iterate over the error log to find all explainable errors.
    matching = 0
    explained = 0
    for error_type in logged_errors:
        for error in logged_errors[error_type]:
            # We provide an output per category (semantic/syntactic) to get a quick overview of what the
            # given approach report can provide.
            if error["category"] not in report["categories"]:
                # Initialize the error category (count the error we just iterated over).
                report["categories"][error["category"]] = {"detected": 0, "total": 1}
            else:
                # Add the error we just iterated over.
                report["categories"][error["category"]]["total"] += 1
            # Check if our approach dictionary has a pattern for this error type. If not, count it as not detected.
            if error_type not in validation_dict:
                report["errors_not_detected"].append({error_type: error})
            else:
                # For each error we can provide several patterns (e.g., if one corruption can cause several errors).
                # We iterate over those patterns and find the total number of detected instances in the approach report.
                error_sum = 0
                for pattern in validation_dict[error_type]["patterns"]:
                    query = fill_approach_dictionary_placeholders(pattern, error,
                                                                  regex_escape=(validation_dict["type"] == "regex"))
                    error_sum += query_function(data_source, query)
                # If we found at least one matching pattern, we count one instance of an error "matching" a corruption.
                # Additionally, we can "explain" the corresponding number of errors.
                # We also add a "detected" count to the category, and add the error instance to the detected errors.
                if error_sum > 0:
                    matching += 1
                    explained += error_sum
                    report["categories"][error["category"]]["detected"] += 1
                    report["errors_detected"].append({error_type: error})
                else:
                    # Otherwise we add the error instance to the undetected errors.
                    report["errors_not_detected"].append({error_type: error})

    report["_validator"] = validator_name
    # number of errors reported by the validation approach
    report["_validator_errors"] = detected
    # number of errors reported by the validation approach that are related to an introduced corruption.
    report["_explained_errors"] = explained
    # On average, how many errors (in the validation approach) does one corruption cause.
    report["_avg_errors_per_introduced_corruption"] = round(explained / matching if matching > 0 else 1, 2)
    # From the average errors per corruption, estimate how many "true" unexplained errors there are.
    report["_estimated_unexplained_corruptions"] = round(
        (detected - explained) / report["_avg_errors_per_introduced_corruption"], 2)
    # From the estimated number of unexplained errors, estimate precision (How many detected errors are corruptions).
    report["_estimated_precision"] = round(
        matching / (matching + report["_estimated_unexplained_corruptions"]) if matching + report[
            "_estimated_unexplained_corruptions"] > 0 else 0, 2)
    # True recall (how many introduced corruptions can we detect).
    report["_recall"] = round(matching / introduced, 2)
    # Estimated F1 based on true recall and estimated precision.
    if report["_estimated_precision"] + report["_recall"] > 0:
        report["_estimated_f1"] = round(2 * report["_estimated_precision"] * report["_recall"] / (
                    report["_estimated_precision"] + report["_recall"]), 2)
    else:
        report["_estimated_f1"] = 0.0
    # Write to the report location, and also print the report to the console.
    with open(kgcbf_report_location, 'w') as outfile:
        yaml.dump(report, outfile, default_flow_style=False)
    print(yaml.dump(report, allow_unicode=True, default_flow_style=False))
